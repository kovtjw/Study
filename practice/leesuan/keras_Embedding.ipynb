{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 \n",
    "\n",
    "단어를 컴퓨터가 이해하고, 효율적으로 처리할 수 있도록 단어를 벡터화 하는 기술\n",
    "잘 표현된 단어벡터들은 계산이 가능하며, 모델 투입도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인코딩\n",
    "\n",
    "기계는 자연어(영어, 한국어)를 이해할 수 없음 >> 기계가 이해할 수 있도록 숫자등으로 변환해주는 작업이 필요\n",
    "텍스트 처리에서는 정수 인코딩, 원 한 인코딩을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 8, 0, 3, 7, 1, 5, 8, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩\n",
    "## dictionary 이용\n",
    "\n",
    "text = '평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라'\n",
    "token = [x for x in text.split(' ')]\n",
    "unique = set(token)\n",
    "unique = list(unique)\n",
    "\n",
    "\n",
    "token2idx = {}\n",
    "for i in range(len(unique)):\n",
    "    token2idx[unique[i]] = i\n",
    "\n",
    "encode = [token2idx[x] for x in token]\n",
    "print(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'것처럼': 1, '평생': 2, '살': 3, '꿈을': 4, '꾸어라': 5, '그리고': 6, '내일': 7, '죽을': 8, '오늘을': 9, '살아라': 10}\n",
      "[2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text = '평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라'\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])      # 각 단어가 차지하고 있는 인덱스 값이 표현이 된다. \n",
    "print(token.word_index)\n",
    "\n",
    "encoded = token.texts_to_sequences([text])[0]\n",
    "print(encoded)     # [2, 3, 1, 4, 5, 6, 7, 8, 1, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원 핫 인코딩은 정수 인코딩한 결과를 벡터로 변환한 인코딩\n",
    "# 원 핫 인코딩은 전체 단어 개수 만큼의 길이를 가진 배열에 해당 정수를 가진 위치는 1, 나머지는 0을 가진 벡터로 변환\n",
    "\n",
    "import numpy as np\n",
    "one_hot = []\n",
    "for i in range(len(encoded)):\n",
    "    temp = []\n",
    "    for j in range(max(encoded)):\n",
    "        if j == (encoded[i]-1):\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "    one_hot.append(temp)\n",
    "    \n",
    "np.array(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "one_hot = to_categorical(encoded)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Dense,Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 1000\n",
    "max_len = 100\n",
    "(x_train,y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])   # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 20, 47, 111, 439, 2, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 2, 39, 294, 11, 22, 396, 13, 28, 8, 202, 12, 2, 23, 94, 2, 151, 111, 211, 469, 4, 20, 13, 258, 546, 2, 2, 12, 16, 38, 78, 33, 211, 15, 12, 16, 2, 63, 93, 12, 6, 253, 106, 10, 10, 48, 335, 267, 18, 6, 364, 2, 2, 20, 19, 6, 2, 7, 2, 189, 5, 6, 2, 7, 2, 2, 95, 2, 6, 2, 7, 2, 2, 49, 369, 120, 5, 28, 49, 253, 10, 10, 13, 2, 19, 85, 795, 15, 4, 481, 9, 55, 78, 807, 9, 375, 8, 2, 8, 794, 76, 7, 4, 58, 5, 4, 816, 9, 243, 7, 43, 50]\n",
      "부정\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if y_train[i] == 0:\n",
    "        label = '부정'\n",
    "    else:\n",
    "        label = '긍정'\n",
    "        \n",
    "print('{}\\n{}'.format(x_train[i], label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "\n",
    "pad_x_train = pad_sequences(x_train, maxlen=max_len, padding='pre')\n",
    "pad_x_test = pad_sequences(x_test, maxlen=max_len, padding='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "[  2  33   6  22  12 215  28  77  52   5  14 407  16  82   2   8   4 107\n",
      " 117   2  15 256   4   2   7   2   5 723  36  71  43 530 476  26 400 317\n",
      "  46   7   4   2   2  13 104  88   4 381  15 297  98  32   2  56  26 141\n",
      "   6 194   2  18   4 226  22  21 134 476  26 480   5 144  30   2  18  51\n",
      "  36  28 224  92  25 104   4 226  65  16  38   2  88  12  16 283   5  16\n",
      "   2 113 103  32  15  16   2  19 178  32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(pad_x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 32)           32000     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3201      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,201\n",
      "Trainable params: 35,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim = num_words, output_dim = 32, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics =['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 4ms/step - loss: 0.5470 - acc: 0.7153 - val_loss: 0.4043 - val_acc: 0.8152\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3634 - acc: 0.8413 - val_loss: 0.3818 - val_acc: 0.8288\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3193 - acc: 0.8651 - val_loss: 0.3863 - val_acc: 0.8268\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2742 - acc: 0.8901 - val_loss: 0.3990 - val_acc: 0.8206\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2295 - acc: 0.9147 - val_loss: 0.4173 - val_acc: 0.8102\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1851 - acc: 0.9395 - val_loss: 0.4417 - val_acc: 0.8094\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1458 - acc: 0.9614 - val_loss: 0.4713 - val_acc: 0.8042\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1125 - acc: 0.9773 - val_loss: 0.5010 - val_acc: 0.8002\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0841 - acc: 0.9884 - val_loss: 0.5351 - val_acc: 0.8000\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0618 - acc: 0.9951 - val_loss: 0.5763 - val_acc: 0.7936\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(pad_x_train, y_train,\n",
    "                 epochs = 10,\n",
    "                 batch_size = 32,\n",
    "                 validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b2a3e2f2f3becee12495ea02e16c44ac6a87253b3da619be1b2ce1aceff9a27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf270gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
